Data preparation:

The machine learning network, that is used in our solution requires a vector with values between 0 and 1(exclusive) as its input. This vector doesnt have any requirements in his length, though its values have to be exclusively numerical.
However the input of the software are GitHub repositories, which mainly consist of textual values such as the project description or the readme. The only non-textual values are the used filename extensions. I will cover these values later on.
The textual values in the repositories need to be converted to numerical ones in order to fit the learning network. This is the purpose of a module named data preparation. Data preparation gets object representations of git repositories and returns numerical vectors which represent the textual values in an unique way.

The conversion brings many different problems with it. The most obvious way would be to assign an id to every word that is used in any of the repositories. With this ids every word can be saved together with his neighbours, so this group of ids would result in an context for the specific word. This solution would be extremely inefficient and also extremely inaccurate for our problem. Either the words occur in many documents, so they dont help us identifying the type or they are specific enough to dont occur in the same context often enough to come to an occlusion based on these.
We wanted instead to make use of the characteristics of decising words, which have an high impact in classifying repositories. These decisive words often dont need any context. Only the appearance of any of this words in an document can give us a hint on classifying the document. So we decided to only store one id for every word and dont store the context with it, as it would produce high additional costs while only giving low benefits.
In addition to only saving one id per word, we wanted to focus on increasing our efficency further. We did this by declaring a huge amount of words as unimportant. At first we cutted off all words with the least and the most frequency in our document sample. Expecially the words with high frquency rarely have impact on the classification. We ignored about 20 percent of the most frequent words and about .3 percent of the least frequent words. As a second we let our machine learning network learn to classify an repository using the rest of the word ids as elements for the output vector. After learning quite a while we could see which words influenced the outcome most, and â€“ more important -  which influenced the outcome least. This words could be deleted to gain an massive increase in efficiency while only losing little accuracy.

While testing this vector, our peak accuration of the learning network was only about 15% at most. So we decided to focus more on the few numeric values existing in the data. Because we only got arrays of texts in the data, the only numeric values we could get were the lengths of this arrays. While testing with a few values we already got an improvement of accuracy to 20%.
