<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Experiment Results &#8212; Gi-Project 1.3 documentation</title>
    
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.3',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Code Documentation" href="code.html" />
    <link rel="prev" title="Introducing the learning mechanism" href="explanation.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="code.html" title="Code Documentation"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="explanation.html" title="Introducing the learning mechanism"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Gi-Project 1.3 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="experiment-results">
<h1>Experiment Results<a class="headerlink" href="#experiment-results" title="Permalink to this headline">¶</a></h1>
<div class="section" id="test-and-validation-that-the-network-is-working">
<h2>Test and Validation that the network is working<a class="headerlink" href="#test-and-validation-that-the-network-is-working" title="Permalink to this headline">¶</a></h2>
<p>To test that our implementation we let the network learn with the MNIST dataset. We used the dataset provided in the Tensorlow API.
Every machine learning program should perform well with MNIST. So if our program is learning this dataset then it is
most likely that we implemented it right.</p>
<div class="section" id="rbm">
<h3>RBM<a class="headerlink" href="#rbm" title="Permalink to this headline">¶</a></h3>
<p>The <code class="xref py py-class docutils literal"><span class="pre">RBM</span></code> got first tested with the MNIST dataset provided by <a class="reference external" href="https://www.tensorflow.org/tutorials/mnist/beginners/">google</a>.
After the training we couldn&#8217;t test the accuracy because the unsupervised learning network didn&#8217;t knew which value in the solution vector
corresponds to which number in the dataset.
It just could see the differences. But with the getImage function from <a class="reference external" href="https://github.com/blackecho/Deep-Learning-TensorFlow/blob/master/yadlt/utils/utilities.py">GitHub</a>
we got a nice representation of our learned states.</p>
</div>
<div class="section" id="dbn">
<h3>DBN<a class="headerlink" href="#dbn" title="Permalink to this headline">¶</a></h3>
<p>The <code class="xref py py-class docutils literal"><span class="pre">DBN</span></code> was also tested by the mnist dataset. We succeeded in learning the MNIST data up to 92% accuracy.
That was the prove that it worked. We first pretrained the network and then used the standard training process as described in
the <a class="reference external" href="https://www.tensorflow.org/tutorials/mnist/pros/">Tensorflow tutorials</a>.</p>
</div>
</div>
<div class="section" id="the-idea">
<h2>The idea<a class="headerlink" href="#the-idea" title="Permalink to this headline">¶</a></h2>
<p>The task was to write an automated classifier for GitHub repositorys.</p>
<p>Our idea was it to use a unsupevised network. We found out that you can stack multiple restricted Boltzmann machines to form a
Deep Belief network which has a much better problem solving ability then a single RBM. But we needed a discriminative model. That&#8217;s why we classified
some repositories by hand. With this data set we could finetune the DBN and then use the DBN to generate new training data and continue the training.</p>
<p>For your network we used Tensorflow with numpy to build the network. The introduction on how to use them can be found on the
<a class="reference internal" href="introduction.html"><span class="doc">Get Started with Tensorflow</span></a> page. All testresults are visualized with Tensorboard.</p>
<p>Most of the general ideas and informations are from Youtube. [youtubeDeep]</p>
</div>
<div class="section" id="first-experiments">
<h2>First Experiments<a class="headerlink" href="#first-experiments" title="Permalink to this headline">¶</a></h2>
<p>Our first run with only 4700 unlabeled datasets wasn&#8217;t really succefull. The hyperparameters were as followed:</p>
<table border="1" class="docutils">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Hyperparameter</th>
<th class="head">Values 1</th>
<th class="head">Values 2</th>
<th class="head">Values 3</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Epochs</td>
<td>100</td>
<td>100</td>
<td>100</td>
</tr>
<tr class="row-odd"><td>Batchsize</td>
<td>10</td>
<td>100</td>
<td>100</td>
</tr>
<tr class="row-even"><td>learningrate</td>
<td>0.1</td>
<td>0.01</td>
<td>0.005</td>
</tr>
<tr class="row-odd"><td>Gibbssamplingsteps</td>
<td>1</td>
<td>3</td>
<td>5</td>
</tr>
<tr class="row-even"><td>Momentumterm</td>
<td>0.5</td>
<td>0.9</td>
<td>0.9</td>
</tr>
<tr class="row-odd"><td>Weightdecay</td>
<td>0.0001</td>
<td>0.0001</td>
<td>0.0001</td>
</tr>
</tbody>
</table>
<p>Each layer is trained with the same 3 values.</p>
<p>Network structure: [1370, 500, 500, 1500, 7]</p>
<p>Accuracy got never over 16%. Sometimes way worse. But the weights stayed small which we interpreted as a good sign.
The choice of Opitmizer and learning rate for the finetuning part didn&#8217;t had any effect on the testresults.</p>
<p>Next we tried it with almost the same parameters but this time with 31700 unlabeled data sets. The initialization was long and the
input vector got up to 7300. The program took just 3GB but after the first epoch the memory usage doubled. We decided to
shorten our input vector.</p>
<p>Every word which happens to occured to few or too much is cut out of the vector.</p>
<p>After this we ended up with an input vector with the size of 1636. The hyperparameters for the third test were as followed:</p>
<table border="1" class="docutils">
<colgroup>
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Hyperparameter</th>
<th class="head">Values 1</th>
<th class="head">Values 2</th>
<th class="head">Values 3</th>
<th class="head">Values 4</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Epochs</td>
<td>10</td>
<td>50</td>
<td>50</td>
<td>50</td>
</tr>
<tr class="row-odd"><td>Batchsize</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
</tr>
<tr class="row-even"><td>learningrate</td>
<td>0.1</td>
<td>0.01</td>
<td>0.005</td>
<td>0.001</td>
</tr>
<tr class="row-odd"><td>Gibbssamplingsteps</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>4</td>
</tr>
<tr class="row-even"><td>Momentumterm</td>
<td>0.5</td>
<td>0.9</td>
<td>0.9</td>
<td>0.9</td>
</tr>
<tr class="row-odd"><td>Weightdecay</td>
<td>0.0001</td>
<td>0.0001</td>
<td>0.0002</td>
<td>0.0002</td>
</tr>
</tbody>
</table>
<p>Network structure: [1636, 500, 500, 1500, 7]</p>
<p>We tested all possible optimizers, network structures and hyperparameters and came to the conclusion that single words aren&#8217;t
usefull for a classification problem if they are used without a context.</p>
</div>
<div class="section" id="new-inputvector">
<h2>New Inputvector<a class="headerlink" href="#new-inputvector" title="Permalink to this headline">¶</a></h2>
<p>So we tried a new short input vector. Which encoded the following things:</p>
<ol class="arabic simple">
<li>Number of files in the repository</li>
<li>Number of commits</li>
<li>Number of open issues</li>
<li>Number of closed issues</li>
<li>Number of closed issues</li>
<li>Number of Users who left a comment</li>
<li>Number of Users who commited something</li>
</ol>
<p>Every of these parameters was normalized relative to the number maximal possible number they could have which is 1000.
Because the GitHub API only returns the first 1000 issues, comments and so on.
But we observed that the network couldn&#8217;t even learn a small dataset. To prove this we let the network train with our
supervised dataset and test every epoch how many of them it could classify correctly.
We never came above the 21% accuracy rate. That&#8217;s why we reworked the vector again. This time we made the value relative
to the other values in the same repository because we expected that all of the given parameters are also dependent on how
big the repository is.</p>
<p>New vector:</p>
<ol class="arabic simple">
<li>Number of Files / the number of maximal possible files</li>
<li>Number of comments / Number of commits</li>
<li>Number of open issues / number of closed issues</li>
<li>Number of authors / number of files</li>
<li>Number of users who commited / number of files</li>
</ol>
<p>But still the network couldn&#8217;t even find a relatively good solution for all our 300 trainsets.</p>
</div>
<div class="section" id="new-normalization-methods">
<h2>New Normalization Methods<a class="headerlink" href="#new-normalization-methods" title="Permalink to this headline">¶</a></h2>
<p>Our hypotheses is that we chose a poor input vector. The network could&#8217;t tell 300 networks apart by the presented values.</p>
<p>That&#8217;s why we tried to find a better fitting one. If the network could tell our 300 vectors apart
it probably could learn more and generalise this idea.</p>
<p>The new vector we tested uses almost the same input values but this time we weren&#8217;t trying to make them relative to each other.
We just used the absolute value but we normalized the whole vector later on.</p>
<p>Inputvalues:
#. Number of comments
#. Number of commits
#. Number of open issues
#. Number of closed issues
#. Number of users who commented
#. Number of users who commited</p>
<p>We tried a new method of normalizing the values between 1 and 0.
We used the <code class="xref py py-meth docutils literal"><span class="pre">tensorflow.nn.l2_normalize()</span></code> function. The trainingresults with a [6, 50, 100, 200, 400, 7] net
were slightly over our previously tested ones. we got up to 33% accuracy.</p>
<p>The shown graphs present the training progress of this netowork. We used a exponential decaying learning rate with a <code class="xref py py-class docutils literal"><span class="pre">ProximalAdagradOptimizer</span></code>.
The l1 = 0.0001 and l2 0.001. Pretraining was with the following paramenters:</p>
<table border="1" class="docutils">
<colgroup>
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Hyperparameter</th>
<th class="head">Values 1</th>
<th class="head">Values 2</th>
<th class="head">Values 3</th>
<th class="head">Values 4</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Epochs</td>
<td>10</td>
<td>45</td>
<td>10</td>
<td>10</td>
</tr>
<tr class="row-odd"><td>Batchsize</td>
<td>10</td>
<td>10</td>
<td>10</td>
<td>10</td>
</tr>
<tr class="row-even"><td>learningrate</td>
<td>0.1</td>
<td>0.01</td>
<td>0.001</td>
<td>0.0001</td>
</tr>
<tr class="row-odd"><td>Gibbssamplingsteps</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>4</td>
</tr>
<tr class="row-even"><td>Momentumterm</td>
<td>0.5</td>
<td>0.9</td>
<td>0.9</td>
<td>0.9</td>
</tr>
</tbody>
</table>
<img alt="_images/finetuning_normalized_2.png" src="_images/finetuning_normalized_2.png" />
<img alt="_images/finetuning_normalized_2_loss.png" src="_images/finetuning_normalized_2_loss.png" />
<img alt="_images/finetuning_normalized_2_lr.png" src="_images/finetuning_normalized_2_lr.png" />
<p>Keep in mind that the graph is smoothed by Tensorboard. That&#8217;s why the 30% accuracy isn&#8217;t visible.</p>
<p>During the pretraining we could observe that the first networks had really big weights and the following RBM&#8217;s had too small weights.
That&#8217;s why we changed the weight decay mechanic. Each entry in weight_dacy parameter corresponds to one RBM network.
That allowed us to tweak the weight decay for every RBM independently.
With this change the networks had a better weight development. The highest or lowest weights have a good initial value for the finetuning as
it can be seen here:</p>
<img alt="_images/pretraining_max_weights_normalized_2.png" src="_images/pretraining_max_weights_normalized_2.png" />
<img alt="_images/pretraining_min_weights_normalized_2.png" src="_images/pretraining_min_weights_normalized_2.png" />
<p>The graphs are saved in logs data_normalized_2. With Tensorboard you can display the graphs yourself, together with additional
information about the training. (<a class="reference internal" href="introduction.html"><span class="doc">Get Started with Tensorflow</span></a>)</p>
<p>We also tested to shift the values of our training_set with a mean of 1 and
variance of 0.
In <a class="reference external" href="http://r2rt.com/implementing-batch-normalization-in-tensorflow.html">this tutorial</a> they used this normalization technique to normalize
the output of every layer in the neural network. We tried to use it as a way to normalize our input data into the first layer.
In our testcase with the same pretraining as done before, only with differently normalized data, hadn&#8217;t any effect on the accuracy.</p>
</div>
<div class="section" id="other-changes">
<h2>Other changes<a class="headerlink" href="#other-changes" title="Permalink to this headline">¶</a></h2>
<p>Throughout the testing of the project we always did change parts of it here and there. But because we initially hadn&#8217;t a working network
we couldn&#8217;t determine if these changes were more or less useful. Simply because it had no effect on the networks accuracy.</p>
<p>The learningrate</p>
<p>The learning rate during pretraining was initially 0.001 - 0.00001 divided by the batch size. This value was originally from our tests with the
MNIST dataset. With a higher learning rate the loss couldn&#8217;t decrement because the learning rate was too high.</p>
<p>After some time we decided to start with higher learningrate and degrade them over time. The pretraining results were much better
because the change in the loss was higher and most of the networks had an 20% accuracy after the first finetuning epoch.</p>
<p>Network topology</p>
<p>The network shrunk naturally after we thinned the input vector.
But during the testing process we tried different topologies:</p>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt>[1600, 500, 500, 2000, 7]</dt>
<dd>The vector is transposed into a smaller dimension where features are extrated. The second last layer then represents a vector with
2000 features. The supervised training can now extract the features it needs to make the classification. The network learns more features
than it needs but we can be sure the right one will be in there. This technique is inspired by the MNIST approach with pretraining.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>[1600, 700, 500, 200, 500, 7000, 20000, 7]</dt>
<dd>This is a huge network. But it can also be done with a smaller input vector and therefore smaller interior layers.
This topology is inspired by autoencoders. First the input value is slowly compressed into a smaller vector.
The second half of the network then reconstructs the input and gives an prediction.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>[6, 200, 100, 50, 25, 7] or [1600, 800, 400, 200, 100, 7]</dt>
<dd>This is the standard way of composing a neural network. The First layer is either turned into a bigger one or directly transformed
into a smaller vector. The first version had some problems because the if we choose the second layer to big the weight tended to explode in
the first weight matrix. That&#8217;s why we used the above method to slowly higher the layer size.s</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>[6, 75, 7]</dt>
<dd>This is the shallow version of a network. We tried this version to test if maybe the additional layers lower the accuracy of our network
because they may not be needed.</dd>
</dl>
</li>
</ol>
<p>Neurontypes</p>
<p>We started with a simple sigmoid activation function and a softmax layer at the last layer.
But in some literature and forums was suggested that the reLU neuron can improve the performance of a network by reducing the
effect vanishing gradient.
We also added a dropout rate of 50% to the network to prevent overfitting because we had just a small training set.</p>
<p>The training set</p>
<p>Because we had just so few training examples we early on decided to use pretraining to shorten the time our network needs to train
and maybe prevent it from overfitting.
Another idea to prevent overfitting is to let the network predict classes for the unlabeled data and add them to the supervised trainingset.
This idea can be seen in <a class="reference internal" href="code.html#Main.fit_dbn" title="Main.fit_dbn"><code class="xref py py-meth docutils literal"><span class="pre">Main.fit_dbn()</span></code></a>. It&#8217;s not used at the moment because the network itself isn&#8217;t learning properly.</p>
<p>Optimizers and errorfunctions</p>
<p>We tetsted most of the available optimizers of Tensorflow. The obvious gradient decent algorithm was unstable when we didn&#8217;t used the
prefect hyperparameters. Additionally the weights tended to overfit. The AdagradOptimizer got the most stable learning progress.
The <code class="xref py py-class docutils literal"><span class="pre">ProximalAdagradOptimizer</span></code> and <code class="xref py py-class docutils literal"><span class="pre">ProximalGradientDecentOptimizer</span></code> can be used as their not proximal counter part but
they offer additional build-in l1 and l2 regulation. So they prevented the network effectively from too high values.</p>
<p>As an error function we used the <code class="xref py py-meth docutils literal"><span class="pre">tensorflow.nn.softmax_cross_entropy_with_logits()</span></code>. This allows just one of the neurons to be active.
An alternative would have been the cross entropy as the sum of the squared error. This would have allowed multiple neurons to be active at
the same time. But our trainingset consisted of only one class solutions.
So we use the softmax version as default.</p>
<p>Pretraining</p>
<p>We varied the pretraining a little bit but we always stuck to the same plan of decaying the learning rate over time and meanwhile
increasing the number of gibbs sampling steps.
We tried to set the learning rate of the third trainingstep to 0.1. The network should use this as an opportunity to escape the
local minimas it might be stuck in. But instead it just provoked higher weights values and a worse loss value. Then when the next training
step started the network got back to it&#8217;s state after the second training iteration.
One change that we kept is to set the initial momentum term to 0.5 for a short amount of time
we couldn&#8217;t really detect a difference.
One bigger change was to use binary states as input for all networks. Previously we just turned the input for the first layer
into binary digits. But because we had the theory that the network is stuck in a local minima the stochastic natur of pretraining
with binary states should prevent this.</p>
<div class="section" id="evaluation">
<h3>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h3>
<p>The concept of pretraining a Deep Neural Network with RBM&#8217;s and then finetune it with backpropagation proofed to be ineffective.
With the modular development the network could be easily modified. Also was the Tensorflow API easy to understand and to use.
That gave us the chance to test different input vectors,
neuron type, optimizers and learning rates. But none of the experiments performed well in our test setups.</p>
<p>The network itself was tested with the MNIST dataset. So it&#8217;s capability of learning complex interactions with only raw pixel
is proofed.
We expect the error either in the input vector or in the chosen structure of our network. For example we couldn&#8217;t find a
working network topology. Mostlikely it is a combination of all the above.</p>
<p>This high complexity makes it extremely difficult to find a solution. In addition to that the field of Deep Neural Networks
is a pretty new. Many different theories in how to interpret and improve their performance are available. Most of them
can&#8217;t be proven mathematically. We had only the possibility to try out different things that worked in other networks and test
them in our.
Also is it difficult to interpret the output of the network. The low accuracy for example can be a result of a poor input vector,
poor trainingsdata or it can simply be stuck in a bad local minima.</p>
<p>To improve the performace we have different options:
#. The network would defently profit from more trainingdata because it helps generalising the problem.
#. We could use a more complex network like a convolutional network which can find solutions for more complex tasks.
#. If we retrieve more information from the GitHub API we could find a better suited input vector. Additionally we could</p>
<blockquote>
<div>perform something like Principal Component Analysis (PCA) to filter with value has  a real impact on our classification problem.</div></blockquote>
<ol class="arabic simple">
<li><dl class="first docutils">
<dt>The normalisation could also be improved. The values should be between 0 and 1 which our method does. But most of the</dt>
<dd>values getting to small which isn&#8217;t optimal for the network.</dd>
</dl>
</li>
</ol>
<p>All in all there are many possible ways to improve the classification ability of our network. But it is really difficult to tell
which should be used.</p>
<table class="docutils citation" frame="void" id="youtubedeep" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[youtubeDeep]</td><td><a class="reference external" href="https://www.youtube.com/channel/UC9OeZkIwhzfv-_Cb7fCikLQ">https://www.youtube.com/channel/UC9OeZkIwhzfv-_Cb7fCikLQ</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Experiment Results</a><ul>
<li><a class="reference internal" href="#test-and-validation-that-the-network-is-working">Test and Validation that the network is working</a><ul>
<li><a class="reference internal" href="#rbm">RBM</a></li>
<li><a class="reference internal" href="#dbn">DBN</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-idea">The idea</a></li>
<li><a class="reference internal" href="#first-experiments">First Experiments</a></li>
<li><a class="reference internal" href="#new-inputvector">New Inputvector</a></li>
<li><a class="reference internal" href="#new-normalization-methods">New Normalization Methods</a></li>
<li><a class="reference internal" href="#other-changes">Other changes</a><ul>
<li><a class="reference internal" href="#evaluation">Evaluation</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="explanation.html"
                        title="previous chapter">Introducing the learning mechanism</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="code.html"
                        title="next chapter">Code Documentation</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/results.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="code.html" title="Code Documentation"
             >next</a> |</li>
        <li class="right" >
          <a href="explanation.html" title="Introducing the learning mechanism"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Gi-Project 1.3 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright None.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.1.
    </div>
  </body>
</html>